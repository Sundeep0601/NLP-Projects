{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "176bfe63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.44.0-py3-none-any.whl (9.5 MB)\n",
      "     ---------------------------------------- 9.5/9.5 MB 6.4 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub<1.0,>=0.23.2\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl (417 kB)\n",
      "     -------------------------------------- 417.5/417.5 kB 5.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sundeep\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sundeep\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (1.22.1)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.4.4-cp39-none-win_amd64.whl (286 kB)\n",
      "     -------------------------------------- 286.1/286.1 kB 8.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Collecting tokenizers<0.20,>=0.19\n",
      "  Downloading tokenizers-0.19.1-cp39-none-win_amd64.whl (2.2 MB)\n",
      "     ---------------------------------------- 2.2/2.2 MB 6.7 MB/s eta 0:00:00\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "     -------------------------------------- 177.6/177.6 kB 5.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sundeep\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Installing collected packages: safetensors, fsspec, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed fsspec-2024.6.1 huggingface-hub-0.24.5 safetensors-0.4.4 tokenizers-0.19.1 transformers-4.44.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script huggingface-cli.exe is installed in 'C:\\Users\\Sundeep\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script transformers-cli.exe is installed in 'C:\\Users\\Sundeep\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fa40cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1876e85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "             \"Three years later, the coffin was still full of Jello.\",\n",
    "             \"The fish dreamed of escaping the fishbowl and into the toilet where he saw his friend go.\",\n",
    "             \"Going to gym and being fit is necessary for all.\",\n",
    "             \"Standing on one's head at job interviews forms a lasting impression.\",\n",
    "             \"It took him a month to finish the meal.\",\n",
    "             \"Having a meal on time is a good habit.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "953e985b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e88b8c26844d44ae26abbd651bf948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/399 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sundeep\\AppData\\Roaming\\Python\\Python39\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Sundeep\\.cache\\huggingface\\hub\\models--sentence-transformers--bert-base-nli-mean-tokens. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc013f8e35ea44ddb5ed86b66f7528b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d18d67cef43446e4a5a5f64101dd8078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf1708abc67e4c379816be1a12f07d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b47cb90ccd401c8924f800bb883377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df39b9290bed4eb7ac1d235ce5be6bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sundeep\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9261bbe60cb46b7bd89ca9b30cbb694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'sentence-transformers/bert-base-nli-mean-tokens'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8db541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tokens(sentences, tokenizer):\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    \n",
    "    # encoding all sentences for bert input\n",
    "    for sentence in sentences:\n",
    "        sentence_encoding = tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids.append(sentence_encoding['input_ids'][0])\n",
    "        attention_mask.append(sentence_encoding['attention_mask'][0])\n",
    "    \n",
    "    # stacking all the input_ids and attention_mask along 1 dim\n",
    "    input_ids = torch.stack(input_ids)\n",
    "    attention_mask = torch.stack(attention_mask)\n",
    "    # final shape of input_ids & attention_mask = torch.Size([6, 128]), initially they were list.\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5385179d",
   "metadata": {},
   "source": [
    "Now let's apply mean pooling on last_hidden_state vector of shape torch.Size([6,128,768])\n",
    "to convert it into meaningful sentence embedding. For this we need to create a sentence vector by multiplying the attention_mask with last_hidden_state so that we ignore non-real tokens that mean, ignoring a padding tokens. The final sentence vector will have 768 embeddings for those words where there was 1 else 0 = padding tokens. In order to multiply, we need to expand attention_mask dim by 1 so that both becomes [6,128,768].\n",
    "\n",
    "# Applying mean pooling\n",
    "This pooling operation will take the mean of all token embeddings and compress them into a single 768 vector space — creating a 'sentence vector’.At the same time, we can’t just take the mean activation as is. We need to consider null padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3613486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentence_vector(tokens, model):\n",
    "    \n",
    "    last_hidden_state, pooled_output = model(**tokens, return_dict=False)\n",
    "    attention_mask = tokens['attention_mask'].unsqueeze(-1).expand(last_hidden_state.shape).float()\n",
    "    masked_embeddings = last_hidden_state * attention_mask \n",
    "    summed = torch.sum(masked_embeddings, dim=1) # shape = [6,768]\n",
    "    counts = torch.clamp(attention_mask.sum(dim=1), min=1e-9) # shape = [6,768]\n",
    "    mean_pooled_embedding = summed / counts # shape = [6, 768] i.e. our final sentence vector.\n",
    "\n",
    "    return mean_pooled_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e82e49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(sentences, tokenizer, model):\n",
    "    \n",
    "    sentences_tokens = compute_tokens(sentences, tokenizer)\n",
    "    sentences_embeddings = compute_sentence_vector(sentences_tokens, model)\n",
    "    sentences_embeddings_detached = sentences_embeddings.detach().numpy()\n",
    "    similarity_scores = cosine_similarity([sentences_embeddings_detached[0]], sentences_embeddings_detached[1:])\n",
    "\n",
    "    d = {\n",
    "        'column-1': [sentences[0] for _ in range(len(sentences)-1)],\n",
    "        'column-2': [sent for sent in sentences[1:]],\n",
    "        'scores': similarity_scores[0]\n",
    "    }\n",
    "\n",
    "    output = pd.DataFrame(data=d)\n",
    "\n",
    "    return output\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d682896",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = compute_similarity(sentences, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3551c545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column-1</th>\n",
       "      <th>column-2</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Three years later, the coffin was still full o...</td>\n",
       "      <td>The fish dreamed of escaping the fishbowl and ...</td>\n",
       "      <td>0.330889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Three years later, the coffin was still full o...</td>\n",
       "      <td>Going to gym and being fit is necessary for all.</td>\n",
       "      <td>0.080173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Three years later, the coffin was still full o...</td>\n",
       "      <td>Standing on one's head at job interviews forms...</td>\n",
       "      <td>0.174755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Three years later, the coffin was still full o...</td>\n",
       "      <td>It took him a month to finish the meal.</td>\n",
       "      <td>0.447096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Three years later, the coffin was still full o...</td>\n",
       "      <td>Having a meal on time is a good habit.</td>\n",
       "      <td>0.144813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            column-1  \\\n",
       "0  Three years later, the coffin was still full o...   \n",
       "1  Three years later, the coffin was still full o...   \n",
       "2  Three years later, the coffin was still full o...   \n",
       "3  Three years later, the coffin was still full o...   \n",
       "4  Three years later, the coffin was still full o...   \n",
       "\n",
       "                                            column-2    scores  \n",
       "0  The fish dreamed of escaping the fishbowl and ...  0.330889  \n",
       "1   Going to gym and being fit is necessary for all.  0.080173  \n",
       "2  Standing on one's head at job interviews forms...  0.174755  \n",
       "3            It took him a month to finish the meal.  0.447096  \n",
       "4             Having a meal on time is a good habit.  0.144813  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
